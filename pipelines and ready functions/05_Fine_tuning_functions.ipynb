{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Are u ready to fine-tune ur Model ?**\n",
    "\n",
    "* fine-tuning could be done using many techniques like: \n",
    "\n",
    "* 1. **Grid Search**: involves searching through a predefined set of hyperparameters to find the best combination that optimizes the model's performance. \n",
    "#\n",
    "* 2. **RandomizedSearchCV**: This function, also from scikit-learn, is similar to GridSearchCV but instead of exhaustively searching through all possible combinations of hyperparameters, it samples a fixed number of hyperparameter settings from specified distributions. This can be more efficient when the hyperparameter space is large.\n",
    "#\n",
    "* 3. **Bayesian Optimization**: Bayesian optimization is a more sophisticated approach that uses probabilistic models to predict which hyperparameters are likely to perform well. Libraries like scikit-optimize and hyperopt provide implementations of Bayesian optimization.\n",
    "#\n",
    "* 4. **Cross-Validation Strategies**: Instead of standard k-fold cross-validation, you can use more advanced strategies like stratified k-fold, time series cross-validation, or leave-one-out cross-validation depending on your data's characteristics.\n",
    "#\n",
    "* 5. **Early Stopping**: Instead of running a fixed number of epochs, use early stopping to monitor the model's performance on a validation set and stop training once performance plateaus or starts to degrade.\n",
    "#\n",
    "* 6. **Learning Rate Schedulers**: When training neural networks, learning rate schedulers adjust the learning rate over time to help the model converge more efficiently. Examples include ReduceLROnPlateau and LearningRateScheduler in Keras.\n",
    "#\n",
    "* 7. ***Ensemble Methods**: Combine multiple models to create an ensemble that often performs better than individual models. Techniques like bagging (Bootstrap Aggregating) and boosting (AdaBoost, XGBoost, LightGBM) can be used.\n",
    "#\n",
    "* 9. **Regularization**: Techniques like L1 and L2 regularization can help prevent overfitting by adding penalty terms to the loss function based on the magnitudes of model parameters.\n",
    "#\n",
    "* 10. **Data Augmentation**: In computer vision tasks, data augmentation techniques such as rotation, flipping, and cropping can artificially increase the size of your training dataset, leading to improved generalization.\n",
    "#\n",
    "* 11. **Hyperparameter Libraries**: Libraries like optuna and hyperopt offer more advanced hyperparameter tuning strategies beyond grid and random search, such as Tree-structured Parzen Estimators (TPE) and Gaussian Processes.\n",
    "#\n",
    "* 12. **Visualization and Analysis Tools**: Tools like scikit-learn's validation_curve and learning_curve functions can help you visualize how changing specific hyperparameters or dataset sizes affects model performance.\n",
    "#\n",
    "* 13. **Automated Machine Learning (AutoML)**: Platforms like Auto-sklearn, H2O.ai, and Google AutoML automate the process of hyperparameter tuning and model selection.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "Best Estimator: SVC(C=1, gamma=0.1, kernel='linear')\n",
      "0.24152294576982397 {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.31622776601683794 {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.24152294576982397 {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\n",
      "0.24152294576982397 {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.24152294576982397 {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "0.32914029430219166 {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.2041241452319315 {'C': 1, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.22360679774997896 {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.2041241452319315 {'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
      "0.2041241452319315 {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.2041241452319315 {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "0.22360679774997896 {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "0.22360679774997896 {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n",
      "0.22360679774997896 {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "0.22360679774997896 {'C': 10, 'gamma': 1, 'kernel': 'linear'}\n",
      "0.24152294576982397 {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.22360679774997896 {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "0.22360679774997896 {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def grid_search(X_train, y_train, model, grid_params, cv):\n",
    "    \"\"\"\n",
    "    Perform grid search for hyperparameter tuning using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (array-like): Training input features.\n",
    "        y_train (array-like): Training target labels.\n",
    "        model: The machine learning model to be tuned.\n",
    "        grid_params (dict) or (list of dicts): Hyperparameter grid to search.\n",
    "        cv: Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        best_estimator: The best estimator (model) found during grid search.\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(estimator=model, param_grid=grid_params, cv=cv, return_train_score=True, scoring='neg_mean_squared_error')\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Best Params:\", gs.best_params_)\n",
    "    print(\"Best Estimator:\", gs.best_estimator_)\n",
    "\n",
    "    # Print the results\n",
    "    cvres = gs.cv_results_\n",
    "    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "        print(np.sqrt(-mean_score), params)\n",
    "\n",
    "    return gs.best_estimator_\n",
    "\n",
    "# --------------------------------------- usage example --------------------------------------------\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define hyperparameters and their possible values for grid search\n",
    "grid_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': [0.1, 1, 'scale']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "best_model = grid_search(X_train, y_train, svm_model, grid_params, cv=5)\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Randomized Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'kernel': 'linear', 'gamma': 'scale', 'C': 1.0}\n",
      "Best Estimator: SVC(kernel='linear')\n",
      "0.7359800721939872 {'kernel': 'linear', 'gamma': 0.1, 'C': 0.001}\n",
      "0.22360679774997896 {'kernel': 'linear', 'gamma': 'scale', 'C': 10.0}\n",
      "0.24152294576982397 {'kernel': 'rbf', 'gamma': 1, 'C': 0.1}\n",
      "0.24152294576982397 {'kernel': 'linear', 'gamma': 'scale', 'C': 0.1}\n",
      "0.2041241452319315 {'kernel': 'linear', 'gamma': 'scale', 'C': 1.0}\n",
      "0.22360679774997896 {'kernel': 'rbf', 'gamma': 0.1, 'C': 10.0}\n",
      "0.22360679774997896 {'kernel': 'linear', 'gamma': 0.1, 'C': 100.0}\n",
      "0.32914029430219166 {'kernel': 'linear', 'gamma': 'scale', 'C': 0.01}\n",
      "0.22360679774997896 {'kernel': 'rbf', 'gamma': 'scale', 'C': 1.0}\n",
      "0.24152294576982397 {'kernel': 'rbf', 'gamma': 'scale', 'C': 100.0}\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def randomized_search(X_train, y_train, model, param_dist, cv, n_iter=10):\n",
    "    \"\"\"\n",
    "    Perform randomized search for hyperparameter tuning using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (array-like): Training input features.\n",
    "        y_train (array-like): Training target labels.\n",
    "        model: The machine learning model to be tuned.\n",
    "        param_dist (dict): Hyperparameter distributions to sample from.\n",
    "        cv: Number of cross-validation folds.\n",
    "        n_iter: Number of parameter settings that are sampled.\n",
    "\n",
    "    Returns:\n",
    "        best_estimator: The best estimator (model) found during randomized search.\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=n_iter, cv=cv, return_train_score=True, scoring='neg_mean_squared_error')\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(\"Best Params:\", random_search.best_params_)\n",
    "    print(\"Best Estimator:\", random_search.best_estimator_)\n",
    "\n",
    "    # Print the results\n",
    "    cvres = random_search.cv_results_\n",
    "    for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "        print(np.sqrt(-mean_score), params)\n",
    "\n",
    "    return random_search.best_estimator_\n",
    "\n",
    "\n",
    "# --------------------------------------- usage example --------------------------------------------\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define hyperparameters and their distributions for randomized search\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 2, 6),  # Sample values from a logarithmic range between 10^-3 and 10^2\n",
    "    'kernel': ['linear', 'rbf'],  # Sample values from these kernel options\n",
    "    'gamma': [0.1, 1, 'scale']  # Sample values from these specific gamma values\n",
    "}\n",
    "\n",
    "\n",
    "# Perform randomized search\n",
    "best_model = randomized_search(X_train, y_train, svm_model, param_dist, cv=5, n_iter=10)\n",
    "\n",
    "# Fit the best model on the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
