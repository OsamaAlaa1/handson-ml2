{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Are u Ready to Get the Data u need ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First: there are 3 ways to get the data:** \n",
    "* Download The Data. \n",
    "* Web Scraping. \n",
    "* Using API.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download the Data**\n",
    "* Choose the public dataset you need and download by passing it's link and file format \n",
    "\n",
    "\n",
    "\n",
    "* Popular open data repositories\n",
    "\n",
    "    * [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    "    * [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "    * [Amazon’s AWS datasets](https://registry.opendata.aws/)\n",
    "#\n",
    "* Meta portals (they list open data repositories)\n",
    "\n",
    "    * [Data Portals](http://dataportals.org/)\n",
    "    * [OpenDataMonitor](http://opendatamonitor.eu/)\n",
    "    * [Quandl](http://quandl.com/)\n",
    "#\n",
    "* Other pages listing many popular open data repositories\n",
    "\n",
    "    * [Wikipedia’s list of Machine Learning datasets](https://homl.info/9)\n",
    "    * [Quora.com](https://homl.info/10)\n",
    "    * [The datasets subreddit](https://www.reddit.com/r/datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "import requests\n",
    "\n",
    "def download_dataset(dataset_link, save_path):\n",
    "    \"\"\"\n",
    "    Download a dataset file from a given URL and save it to a specified local path.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_link (str): The URL of the dataset file to be downloaded.\n",
    "        save_path (str): The local path where the downloaded dataset file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the dataset link\n",
    "        response = requests.get(dataset_link)\n",
    "        response.raise_for_status()  # Raise an exception if the response status code indicates an error\n",
    "\n",
    "        # Open the specified save_path file in binary write mode\n",
    "        with open(save_path, 'wb') as file:\n",
    "            # Write the content of the response to the file\n",
    "            file.write(response.content)\n",
    "\n",
    "        print(\"Download complete.\")  # Print a message indicating successful download\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)  # Print an error message if there's an exception during the request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extract data from zip folder function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shutil\n",
    "#!pip install os\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def extract_archive(archive_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extract the contents of an archive (zip, tgz , tar.gz, tar.bz2, tar.xz) to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        archive_path (str): The path to the archive file to be extracted.\n",
    "        extract_to (str): The directory where the contents of the archive will be extracted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine the archive format based on the file extension\n",
    "        _, extension = os.path.splitext(archive_path)\n",
    "\n",
    "        if extension == '.zip':\n",
    "            # Open the zip file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'zip')\n",
    "        elif extension == '.tar.gz' or extension == '.tgz':\n",
    "            # Open the tar.gz file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'gztar')\n",
    "        elif extension == '.tar.bz2':\n",
    "            # Open the tar.bz2 file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'bztar')\n",
    "        elif extension == '.tar.xz':\n",
    "            # Open the tar.xz file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'xz')\n",
    "        else:\n",
    "            print(\"Error: Unsupported archive format.\")\n",
    "\n",
    "        print(\"Extraction complete.\")  # Print a message indicating successful extraction\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)  # Print an error message if there's an exception during extraction\n",
    "\n",
    "# Example usage for various archive formats\n",
    "archive_path = \"path/to/your/archive.tar.gz\"  # Replace with the path to your archive file\n",
    "extract_to = \"path/to/extract/folder\"  # Replace with the directory where you want to extract\n",
    "\n",
    "extract_archive(archive_path, extract_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping**\n",
    "* **when it comes to web scraping there are many powerful packages to do it like :** \n",
    "\n",
    "    * BeautifulSoup (Python).\n",
    "    * Selenium Webdriver (Java, Python and Ruby).\n",
    "    * Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BeautifulSoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
