{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Are u Ready to Get the Data u need ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First: there are 4 ways to get the data:** \n",
    "* Download The Data. \n",
    "* Web Scraping. \n",
    "* mySQL Database. \n",
    "* Using API.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Download the Data**\n",
    "* Choose the public dataset you need and download by passing it's link and file format \n",
    "\n",
    "\n",
    "\n",
    "* Popular open data repositories\n",
    "\n",
    "    * [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n",
    "    * [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "    * [Amazon’s AWS datasets](https://registry.opendata.aws/)\n",
    "#\n",
    "* Meta portals (they list open data repositories)\n",
    "\n",
    "    * [Data Portals](http://dataportals.org/)\n",
    "    * [OpenDataMonitor](http://opendatamonitor.eu/)\n",
    "    * [Quandl](http://quandl.com/)\n",
    "#\n",
    "* Other pages listing many popular open data repositories\n",
    "\n",
    "    * [Wikipedia’s list of Machine Learning datasets](https://homl.info/9)\n",
    "    * [Quora.com](https://homl.info/10)\n",
    "    * [The datasets subreddit](https://www.reddit.com/r/datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "import requests\n",
    "\n",
    "def download_dataset(dataset_link, save_path):\n",
    "    \"\"\"\n",
    "    Download a dataset file from a given URL and save it to a specified local path.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_link (str): The URL of the dataset file to be downloaded.\n",
    "        save_path (str): The local path where the downloaded dataset file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the dataset link\n",
    "        response = requests.get(dataset_link)\n",
    "        response.raise_for_status()  # Raise an exception if the response status code indicates an error\n",
    "\n",
    "        # Open the specified save_path file in binary write mode\n",
    "        with open(save_path, 'wb') as file:\n",
    "            # Write the content of the response to the file\n",
    "            file.write(response.content)\n",
    "\n",
    "        print(\"Download complete.\")  # Print a message indicating successful download\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)  # Print an error message if there's an exception during the request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extract data from zip folder function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shutil\n",
    "#!pip install os\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def extract_archive(archive_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extract the contents of an archive (zip, tgz , tar.gz, tar.bz2, tar.xz) to a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "        archive_path (str): The path to the archive file to be extracted.\n",
    "        extract_to (str): The directory where the contents of the archive will be extracted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine the archive format based on the file extension\n",
    "        _, extension = os.path.splitext(archive_path)\n",
    "\n",
    "        if extension == '.zip':\n",
    "            # Open the zip file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'zip')\n",
    "        elif extension == '.tar.gz' or extension == '.tgz':\n",
    "            # Open the tar.gz file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'gztar')\n",
    "        elif extension == '.tar.bz2':\n",
    "            # Open the tar.bz2 file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'bztar')\n",
    "        elif extension == '.tar.xz':\n",
    "            # Open the tar.xz file and extract its contents\n",
    "            shutil.unpack_archive(archive_path, extract_to, 'xz')\n",
    "        else:\n",
    "            print(\"Error: Unsupported archive format.\")\n",
    "\n",
    "        print(\"Extraction complete.\")  # Print a message indicating successful extraction\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)  # Print an error message if there's an exception during extraction\n",
    "\n",
    "# Example usage for various archive formats\n",
    "archive_path = \"path/to/your/archive.tar.gz\"  # Replace with the path to your archive file\n",
    "extract_to = \"path/to/extract/folder\"  # Replace with the directory where you want to extract\n",
    "\n",
    "extract_archive(archive_path, extract_to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping**\n",
    "* **when it comes to web scraping there are many powerful packages to do it like :** \n",
    "\n",
    "    * BeautifulSoup (Python).\n",
    "    * Selenium Webdriver (Java, Python and Ruby).\n",
    "    * Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BeautifulSoup:**\n",
    "* For simple HTML/XML parsing tasks and the website doesn't heavily rely on JavaScript, Beautiful Soup is a quick and easy choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# BeautifulSoup Scraper Function\n",
    "def scrape_with_beautifulsoup(url,tag,element = 'article'):\n",
    "    \"\"\"\n",
    "    Scrape specific tag from a website using Beautiful Soup 4.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tag content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the tag empty array\n",
    "    tag_contents = []\n",
    "\n",
    "    # return the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # get the html page content \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # iterate over all elements \n",
    "    for elm  in soup.find_all(element):\n",
    "\n",
    "        # find specific tag inside this element \n",
    "        content = elm.find(tag).text\n",
    "\n",
    "        # then add this cotent to the list \n",
    "        tag_contents.append(content)\n",
    "    \n",
    "    # return all contents\n",
    "    return tag_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Selenium**\n",
    "* If the website relies heavily on JavaScript, requires user interactions, or has dynamic content, Selenium is the way to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Selenium Scraper Function\n",
    "def scrape_with_selenium(url,tag,element ='article'):\n",
    "    \"\"\"\n",
    "    Scrape specific tag from a website using Beautiful Soup 4.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tag content.\n",
    "    \"\"\"\n",
    "    # define the tag empty array\n",
    "    tag_contents = []\n",
    "\n",
    "    driver = webdriver.Chrome()  # You need to have the Chrome WebDriver installed\n",
    "    driver.get(url)\n",
    "    \n",
    "    for elm in driver.find_elements_by_css_selector(element):\n",
    "        content = elm.find_element_by_css_selector(tag).text\n",
    "        tag_contents.append(content)\n",
    "    \n",
    "    driver.quit()\n",
    "    return tag_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy**\n",
    "* Scrapy is a powerful framework that supports efficient crawling, data extraction, and pipeline processing and for larger scraping projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scrapy\n",
    "# Make sure to install the 'scrapy' library if not already installed.\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = \"article_spider\"\n",
    "    \n",
    "    # The 'start_requests' method is called to initialize the starting URLs for the spider.\n",
    "    def start_requests(self):\n",
    "        urls = [\"https://example.com\"]  # Replace with the URL of the website to scrape\n",
    "        \n",
    "        # Iterate over the list of URLs and yield a request for each URL with the 'parse' callback.\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    # The 'parse' method is the default callback function that processes the downloaded response.\n",
    "    def parse(self, response):\n",
    "        article_titles = []  # Initialize a list to store article titles\n",
    "        \n",
    "        # Use CSS selectors to extract article titles from the response's HTML content.\n",
    "        for article in response.css(\"article\"):\n",
    "            title = article.css(\"h2::text\").get()  # Extract the text content of <h2> element\n",
    "            article_titles.append(title)  # Add the extracted title to the list\n",
    "        \n",
    "        return article_titles  # Return the list of extracted article titles\n",
    "\n",
    "# Scrapy Scraper Function\n",
    "def scrape_with_scrapy(url):\n",
    "    \"\"\"\n",
    "    Scrape article titles from a website using Scrapy.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of article titles.\n",
    "    \"\"\"\n",
    "    process = CrawlerProcess()  # Create a CrawlerProcess instance\n",
    "    process.crawl(ArticleSpider, start_urls=[url])  # Initialize the ArticleSpider with start URLs\n",
    "    process.start()  # Start the crawling process\n",
    "    return process.spider_instances[0].parsed_data  # Return the parsed article titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MySQL Database** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python\n",
    "import mysql.connector\n",
    "\n",
    "# function to retrieve data using condition\n",
    "def retrieve_data_from_mysql_condition(host, user, password, database, table, condition_column, condition_value):\n",
    "    \"\"\"\n",
    "    Retrieve data from a MySQL database table based on a specified condition.\n",
    "\n",
    "    Parameters:\n",
    "        host (str): MySQL host name.\n",
    "        user (str): MySQL username.\n",
    "        password (str): MySQL password.\n",
    "        database (str): Name of the MySQL database.\n",
    "        table (str): Name of the table to query.\n",
    "        condition_column (str): Column name for the condition.\n",
    "        condition_value (str): Value for the condition.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples representing the retrieved data rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            database=database\n",
    "        )\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Construct the SQL query dynamically with placeholders for the condition\n",
    "        query = f\"SELECT * FROM {table} WHERE {condition_column} = %s\"\n",
    "        \n",
    "        # Execute the query with the condition value\n",
    "        cursor.execute(query, (condition_value,))\n",
    "        \n",
    "        # Fetch all rows as a list of tuples\n",
    "        retrieved_data = cursor.fetchall()\n",
    "        \n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        return retrieved_data\n",
    "    \n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Error:\", err)\n",
    "        return None\n",
    "\n",
    "\n",
    "# function to retrieve data using query\n",
    "def retrieve_data_from_mysql_query(host, user, password, database, query):\n",
    "    \"\"\"\n",
    "    Retrieve data from a MySQL database table based on a specified condition.\n",
    "\n",
    "    Parameters:\n",
    "        host (str): MySQL host name.\n",
    "        user (str): MySQL username.\n",
    "        password (str): MySQL password.\n",
    "        database (str): Name of the MySQL database.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples representing the retrieved data rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            database=database\n",
    "        )\n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Execute the query with the condition value\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Fetch all rows as a list of tuples\n",
    "        retrieved_data = cursor.fetchall()\n",
    "        \n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        \n",
    "        return retrieved_data\n",
    "    \n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Error:\", err)\n",
    "        return None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
