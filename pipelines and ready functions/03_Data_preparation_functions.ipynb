{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Are u ready to Prepare the data ?**\n",
    "\n",
    "* To make sure that data is clean and in a good shape for ml algorithm u need to follow some steps: \n",
    " \n",
    " * **1. Data cleaning:**\n",
    " \n",
    "   * Fix or remove outliers (optional).\n",
    "   * Fill in missing values (e.g., with zero, mean, medianâ€¦) or drop thei rows (or columns).\n",
    "   #\n",
    "\n",
    "* **2. Feature selection (optional):**\n",
    "  * Drop the attributes that provide no useful information for the task.\n",
    "  #\n",
    "*  **3. Feature engineering, where appropriate:**\n",
    "  * Discretize continuous features\n",
    "  * Decompose features (e.g., categorical, date/time, etc.).\n",
    "  * Add promising transformations of features (e.g., log(x), sqrt(x), x2,etc.).\n",
    "  * Aggregate features into promising new features.\n",
    "    \n",
    "#\n",
    "*  **4. Feature scaling:**\n",
    "  * Standardize or normalize features\n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\conda\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# clean function remove outliers on request and do imputation for the missing values\n",
    "def clean_data(dataframe, outliers=True, fill_missing='mean'):\n",
    "    \"\"\"\n",
    "    Perform data cleaning on the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The DataFrame to clean.\n",
    "        outliers (bool): Whether to perform outlier detection and removal using IQR.\n",
    "        fill_missing (str or numeric): Method to fill missing values ('zero', 'mean', 'median', 'drop'),\n",
    "                                        or a numeric value for custom imputation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned_df = dataframe.copy()  # Create a copy to keep the original data\n",
    "        \n",
    "        # Detect the outliers and remove them for numeric columns\n",
    "        numeric_columns = cleaned_df.select_dtypes(include=[np.number]).columns\n",
    "        if outliers:\n",
    "            Q1 = cleaned_df[numeric_columns].quantile(0.25)\n",
    "            Q3 = cleaned_df[numeric_columns].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            cleaned_df[numeric_columns] = cleaned_df[numeric_columns].applymap(\n",
    "                lambda x: x if (x >= lower_bound[x.name]) and (x <= upper_bound[x.name]) else np.nan\n",
    "            )\n",
    "\n",
    "        # Impute missing values in categorical columns with the most frequent value\n",
    "        categorical_columns = cleaned_df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_columns:\n",
    "            most_frequent_value = cleaned_df[col].mode()[0]\n",
    "            cleaned_df[col].fillna(most_frequent_value, inplace=True)\n",
    "\n",
    "        # Impute missing values in numeric columns using the selected strategy\n",
    "        # Map imputation strategies to corresponding functions\n",
    "        \n",
    "        IMPUTE_STRATEGIES = {\n",
    "        'zero': lambda df, col: df[col].fillna(0),\n",
    "        'mean': lambda df, col: df[col].fillna(df[col].mean()),\n",
    "        'median': lambda df, col: df[col].fillna(df[col].median())}\n",
    "\n",
    "        # drop missing values \n",
    "        if fill_missing == 'drop':\n",
    "            cleaned_df.dropna(inplace=True)\n",
    "\n",
    "        # impute with number \n",
    "        elif isinstance(fill_missing, (int, float)):\n",
    "            cleaned_df[numeric_columns] = cleaned_df[numeric_columns].fillna(fill_missing)\n",
    "\n",
    "        # impute with mean, median, zero\n",
    "        elif fill_missing in IMPUTE_STRATEGIES:\n",
    "            for col in numeric_columns:\n",
    "                cleaned_df[col] = IMPUTE_STRATEGIES[fill_missing](cleaned_df, col)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong, missing imputation strategy! Please enter one of these options: ('zero', 'mean', 'median', 'drop', number)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "\n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2,3 Feature Selection and Engineering - Optional** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop or add new columns on need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Feature Scaling and encoding - preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def preprocess_data(df, save_pipeline=False, pipeline_filename='preprocessing_pipeline.pkl'):\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame by performing one-hot encoding on categorical columns\n",
    "    and scaling numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        save_pipeline (bool): Whether to save the preprocessing pipeline to a file.\n",
    "        pipeline_filename (str): Filename to save the preprocessing pipeline (if save_pipeline is True).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the categorical columns one-hot encoded\n",
    "                    and the numerical columns scaled, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        preprocessed_df = df.copy()  # Create a copy to keep the original data\n",
    "\n",
    "        # separate categorical and numerical columns\n",
    "        categorical_columns = preprocessed_df.select_dtypes(include=['object']).columns\n",
    "        numeric_columns = preprocessed_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        # Define the transformers for categorical and numerical columns\n",
    "        transformers = [\n",
    "            ('categorical', OneHotEncoder(), categorical_columns),\n",
    "            #('categorical', OrdinalEncoder(), categorical_columns), # we don't use ordinal encoder as the categorical column not in order \n",
    "            ('numerical', StandardScaler(), numeric_columns)\n",
    "            #('numerical', MinMaxScaler(), numeric_columns) # we use this if we are sure there are no oulliers \n",
    "        ]\n",
    "\n",
    "        # Create a column transformer to apply transformers to the respective columns\n",
    "        ct = ColumnTransformer(transformers, remainder='passthrough')\n",
    "\n",
    "        # Create a pipeline with the column transformer\n",
    "        pipeline = Pipeline(steps=[('preprocessor', ct)])\n",
    "\n",
    "        # Fit and transform the data using the pipeline\n",
    "        processed_data = pipeline.fit_transform(df)\n",
    "\n",
    "        # Convert the processed data array back to a DataFrame\n",
    "        processed_df = pd.DataFrame(processed_data, columns=ct.get_feature_names_out(input_features=df.columns))\n",
    "        \n",
    "        # Save the pipeline if requested\n",
    "        if save_pipeline:\n",
    "            joblib.dump(pipeline, pipeline_filename)\n",
    "            print(\"Pipeline saved as\", pipeline_filename)\n",
    "        \n",
    "        return processed_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during preprocessing:\", e)\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
