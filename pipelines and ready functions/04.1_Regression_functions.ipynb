{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression Modeling Functions Notebook** \n",
    "* this notebook contains functions and steps for regression modeling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression candidate Models** \n",
    "\n",
    "1. **Linear Regression**: A basic regression model that models the relationship between the independent variables and the target variable using a linear equation.\n",
    "\n",
    "2. **Ridge Regression (L2 Regularization)**: A linear regression model with added L2 regularization to prevent overfitting.\n",
    "\n",
    "3. **Lasso Regression (L1 Regularization)**: Similar to Ridge Regression, but with L1 regularization, which can lead to feature selection by driving some coefficients to exactly zero.\n",
    "\n",
    "4. **ElasticNet Regression**: A combination of Ridge and Lasso, incorporating both L1 and L2 regularization.\n",
    "\n",
    "5. **Polynomial Regression**: Extends linear regression by including polynomial terms of the features to capture non-linear relationships.\n",
    "\n",
    "6. **Decision Tree Regression**: Uses decision tree algorithms to predict the target variable based on the feature values.\n",
    "\n",
    "7. **Random Forest Regression**: An ensemble of decision trees that can handle non-linearity and provide improved performance and generalization.\n",
    "\n",
    "8. **Gradient Boosting Regression**: A boosting technique that builds multiple weak learners (usually decision trees) sequentially, with each one trying to correct the errors of the previous one.\n",
    "\n",
    "9. **XGBoost (Extreme Gradient Boosting)**: A highly optimized gradient boosting framework that often outperforms traditional gradient boosting algorithms.\n",
    "\n",
    "10. **LightGBM**: Another gradient boosting framework that's designed for efficiency and can handle large datasets well.\n",
    "\n",
    "11. **CatBoost**: Yet another gradient boosting library that provides support for categorical features out of the box.\n",
    "\n",
    "12. **Support Vector Regression (SVR)**: Uses support vector machines to find the optimal hyperplane that best fits the data.\n",
    "\n",
    "13. **K-Nearest Neighbors (KNN) Regression**: Predicts the target value based on the average of the K-nearest neighbors' target values.\n",
    "\n",
    "14. **Neural Network Regression**: Utilizes neural networks to model complex relationships between features and target variables.\n",
    "\n",
    "15. **Bayesian Regression**: Incorporates Bayesian principles to estimate the posterior distribution of model parameters and predictions.\n",
    "\n",
    "16. **Huber Regression**: A robust regression technique that's less sensitive to outliers compared to ordinary least squares.\n",
    "\n",
    "17. **Quantile Regression**: Focuses on modeling different quantiles of the target variable, making it useful for understanding the entire distribution.\n",
    "\n",
    "18. **Isotonic Regression**: Preserves the order of the data while modeling the relationship between features and target.\n",
    "\n",
    "Remember that the performance of these models can vary depending on the nature of your data, the problem you're trying to solve, and the amount of data available. It's a good practice to try multiple models, tune their hyperparameters, and evaluate their performance using appropriate metrics before selecting the best one for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 140\n",
      "[LightGBM] [Info] Number of data points in the train set: 80, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 1.872579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "0:\tlearn: 1.4299279\ttotal: 1.56ms\tremaining: 155ms\n",
      "1:\tlearn: 1.4008955\ttotal: 2.43ms\tremaining: 119ms\n",
      "2:\tlearn: 1.3789802\ttotal: 3.37ms\tremaining: 109ms\n",
      "3:\tlearn: 1.3356283\ttotal: 4.18ms\tremaining: 100ms\n",
      "4:\tlearn: 1.3060525\ttotal: 4.81ms\tremaining: 91.4ms\n",
      "5:\tlearn: 1.2725260\ttotal: 5.56ms\tremaining: 87.1ms\n",
      "6:\tlearn: 1.2453403\ttotal: 6.05ms\tremaining: 80.5ms\n",
      "7:\tlearn: 1.2265735\ttotal: 8.14ms\tremaining: 93.7ms\n",
      "8:\tlearn: 1.2047794\ttotal: 8.86ms\tremaining: 89.5ms\n",
      "9:\tlearn: 1.1797702\ttotal: 9.85ms\tremaining: 88.7ms\n",
      "10:\tlearn: 1.1630638\ttotal: 10.9ms\tremaining: 88.5ms\n",
      "11:\tlearn: 1.1444302\ttotal: 11.4ms\tremaining: 83.7ms\n",
      "12:\tlearn: 1.1210697\ttotal: 12.3ms\tremaining: 82.1ms\n",
      "13:\tlearn: 1.1021982\ttotal: 13.4ms\tremaining: 82.5ms\n",
      "14:\tlearn: 1.0874311\ttotal: 14ms\tremaining: 79.1ms\n",
      "15:\tlearn: 1.0654598\ttotal: 14.7ms\tremaining: 76.9ms\n",
      "16:\tlearn: 1.0483617\ttotal: 15.8ms\tremaining: 77.3ms\n",
      "17:\tlearn: 1.0388440\ttotal: 16.7ms\tremaining: 76.2ms\n",
      "18:\tlearn: 1.0215617\ttotal: 17.4ms\tremaining: 74.1ms\n",
      "19:\tlearn: 1.0122048\ttotal: 18.9ms\tremaining: 75.6ms\n",
      "20:\tlearn: 1.0029403\ttotal: 19.6ms\tremaining: 73.8ms\n",
      "21:\tlearn: 0.9936327\ttotal: 20.2ms\tremaining: 71.5ms\n",
      "22:\tlearn: 0.9860498\ttotal: 21.3ms\tremaining: 71.4ms\n",
      "23:\tlearn: 0.9771400\ttotal: 21.9ms\tremaining: 69.4ms\n",
      "24:\tlearn: 0.9696022\ttotal: 22.5ms\tremaining: 67.4ms\n",
      "25:\tlearn: 0.9645248\ttotal: 22.9ms\tremaining: 65.1ms\n",
      "26:\tlearn: 0.9567047\ttotal: 23.3ms\tremaining: 63.1ms\n",
      "27:\tlearn: 0.9489363\ttotal: 23.8ms\tremaining: 61.1ms\n",
      "28:\tlearn: 0.9422919\ttotal: 24.3ms\tremaining: 59.4ms\n",
      "29:\tlearn: 0.9272453\ttotal: 24.8ms\tremaining: 57.9ms\n",
      "30:\tlearn: 0.9202893\ttotal: 25.5ms\tremaining: 56.8ms\n",
      "31:\tlearn: 0.9164443\ttotal: 27.3ms\tremaining: 58.1ms\n",
      "32:\tlearn: 0.9092769\ttotal: 28.6ms\tremaining: 58.1ms\n",
      "33:\tlearn: 0.8968272\ttotal: 30ms\tremaining: 58.2ms\n",
      "34:\tlearn: 0.8917580\ttotal: 30.6ms\tremaining: 56.9ms\n",
      "35:\tlearn: 0.8842960\ttotal: 31.7ms\tremaining: 56.4ms\n",
      "36:\tlearn: 0.8782797\ttotal: 33.1ms\tremaining: 56.3ms\n",
      "37:\tlearn: 0.8735691\ttotal: 33.7ms\tremaining: 54.9ms\n",
      "38:\tlearn: 0.8680113\ttotal: 35.3ms\tremaining: 55.2ms\n",
      "39:\tlearn: 0.8658476\ttotal: 36.1ms\tremaining: 54.1ms\n",
      "40:\tlearn: 0.8609199\ttotal: 36.7ms\tremaining: 52.8ms\n",
      "41:\tlearn: 0.8554435\ttotal: 37.6ms\tremaining: 51.9ms\n",
      "42:\tlearn: 0.8513320\ttotal: 38.5ms\tremaining: 51.1ms\n",
      "43:\tlearn: 0.8445591\ttotal: 39.5ms\tremaining: 50.3ms\n",
      "44:\tlearn: 0.8379782\ttotal: 40.3ms\tremaining: 49.2ms\n",
      "45:\tlearn: 0.8260483\ttotal: 41.1ms\tremaining: 48.3ms\n",
      "46:\tlearn: 0.8234690\ttotal: 45.3ms\tremaining: 51.1ms\n",
      "47:\tlearn: 0.8172912\ttotal: 47.7ms\tremaining: 51.7ms\n",
      "48:\tlearn: 0.8105313\ttotal: 48.9ms\tremaining: 50.9ms\n",
      "49:\tlearn: 0.8065940\ttotal: 50.6ms\tremaining: 50.6ms\n",
      "50:\tlearn: 0.7992109\ttotal: 51.7ms\tremaining: 49.7ms\n",
      "51:\tlearn: 0.7958936\ttotal: 52.6ms\tremaining: 48.6ms\n",
      "52:\tlearn: 0.7939623\ttotal: 53.4ms\tremaining: 47.4ms\n",
      "53:\tlearn: 0.7855361\ttotal: 54.6ms\tremaining: 46.5ms\n",
      "54:\tlearn: 0.7814115\ttotal: 55.4ms\tremaining: 45.3ms\n",
      "55:\tlearn: 0.7793698\ttotal: 60.5ms\tremaining: 47.6ms\n",
      "56:\tlearn: 0.7731389\ttotal: 62.3ms\tremaining: 47ms\n",
      "57:\tlearn: 0.7654427\ttotal: 64.3ms\tremaining: 46.6ms\n",
      "58:\tlearn: 0.7594874\ttotal: 65.6ms\tremaining: 45.6ms\n",
      "59:\tlearn: 0.7554698\ttotal: 66.2ms\tremaining: 44.1ms\n",
      "60:\tlearn: 0.7493835\ttotal: 67ms\tremaining: 42.8ms\n",
      "61:\tlearn: 0.7429545\ttotal: 67.8ms\tremaining: 41.6ms\n",
      "62:\tlearn: 0.7350266\ttotal: 68.6ms\tremaining: 40.3ms\n",
      "63:\tlearn: 0.7273329\ttotal: 70.8ms\tremaining: 39.8ms\n",
      "64:\tlearn: 0.7215382\ttotal: 72.7ms\tremaining: 39.2ms\n",
      "65:\tlearn: 0.7164928\ttotal: 73.5ms\tremaining: 37.9ms\n",
      "66:\tlearn: 0.7116117\ttotal: 75.4ms\tremaining: 37.1ms\n",
      "67:\tlearn: 0.7068739\ttotal: 76.6ms\tremaining: 36ms\n",
      "68:\tlearn: 0.7024122\ttotal: 78.1ms\tremaining: 35.1ms\n",
      "69:\tlearn: 0.6981810\ttotal: 78.9ms\tremaining: 33.8ms\n",
      "70:\tlearn: 0.6908254\ttotal: 79.7ms\tremaining: 32.5ms\n",
      "71:\tlearn: 0.6829731\ttotal: 81.2ms\tremaining: 31.6ms\n",
      "72:\tlearn: 0.6770400\ttotal: 81.8ms\tremaining: 30.2ms\n",
      "73:\tlearn: 0.6744885\ttotal: 84.1ms\tremaining: 29.5ms\n",
      "74:\tlearn: 0.6701897\ttotal: 86.8ms\tremaining: 28.9ms\n",
      "75:\tlearn: 0.6685042\ttotal: 87.8ms\tremaining: 27.7ms\n",
      "76:\tlearn: 0.6621753\ttotal: 88.6ms\tremaining: 26.5ms\n",
      "77:\tlearn: 0.6610665\ttotal: 89.4ms\tremaining: 25.2ms\n",
      "78:\tlearn: 0.6567449\ttotal: 90.5ms\tremaining: 24.1ms\n",
      "79:\tlearn: 0.6549464\ttotal: 91.4ms\tremaining: 22.8ms\n",
      "80:\tlearn: 0.6533742\ttotal: 92ms\tremaining: 21.6ms\n",
      "81:\tlearn: 0.6483918\ttotal: 93ms\tremaining: 20.4ms\n",
      "82:\tlearn: 0.6467947\ttotal: 93.7ms\tremaining: 19.2ms\n",
      "83:\tlearn: 0.6437046\ttotal: 98.6ms\tremaining: 18.8ms\n",
      "84:\tlearn: 0.6379183\ttotal: 100ms\tremaining: 17.7ms\n",
      "85:\tlearn: 0.6365981\ttotal: 102ms\tremaining: 16.7ms\n",
      "86:\tlearn: 0.6349765\ttotal: 104ms\tremaining: 15.5ms\n",
      "87:\tlearn: 0.6302253\ttotal: 105ms\tremaining: 14.3ms\n",
      "88:\tlearn: 0.6290940\ttotal: 105ms\tremaining: 13ms\n",
      "89:\tlearn: 0.6282108\ttotal: 106ms\tremaining: 11.8ms\n",
      "90:\tlearn: 0.6224656\ttotal: 107ms\tremaining: 10.6ms\n",
      "91:\tlearn: 0.6186479\ttotal: 108ms\tremaining: 9.43ms\n",
      "92:\tlearn: 0.6144855\ttotal: 112ms\tremaining: 8.39ms\n",
      "93:\tlearn: 0.6130966\ttotal: 113ms\tremaining: 7.19ms\n",
      "94:\tlearn: 0.6122963\ttotal: 113ms\tremaining: 5.95ms\n",
      "95:\tlearn: 0.6063555\ttotal: 114ms\tremaining: 4.77ms\n",
      "96:\tlearn: 0.6056075\ttotal: 115ms\tremaining: 3.56ms\n",
      "97:\tlearn: 0.6018457\ttotal: 116ms\tremaining: 2.37ms\n",
      "98:\tlearn: 0.5980756\ttotal: 117ms\tremaining: 1.18ms\n",
      "99:\tlearn: 0.5947516\ttotal: 119ms\tremaining: 0us\n",
      "Linear Regression MSE: 0.9759\n",
      "Ridge Regression MSE: 0.9481\n",
      "Lasso Regression MSE: 1.2861\n",
      "ElasticNet Regression MSE: 1.2861\n",
      "Decision Tree Regression MSE: 2.1280\n",
      "Random Forest Regression MSE: 1.3305\n",
      "Gradient Boosting Regression MSE: 1.2564\n",
      "XGBoost Regression MSE: 1.2513\n",
      "LightGBM Regression MSE: 1.0662\n",
      "CatBoost Regression MSE: 0.9560\n",
      "SVR Regression MSE: 1.1075\n",
      "KNN Regression MSE: 1.0096\n",
      "Neural Network Regression MSE: 1.9269\n",
      "Bayesian Regression MSE: 0.9562\n",
      "Huber Regression MSE: 1.0745\n",
      "Quantile Regression MSE: 1.3290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_quantile.py:186: FutureWarning: The default solver will change from 'interior-point' to 'highs' in version 1.4. Set `solver='highs'` or to the desired solver to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "#from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)  # Generating 100 samples with 5 features\n",
    "y = 2*X[:, 0] + 3*X[:, 1] - 1.5*X[:, 2] + np.random.randn(100)  # True target values with added noise\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "def linear_regression(X_train, y_train, X_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Ridge Regression\n",
    "def ridge_regression(X_train, y_train, X_test):\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Lasso Regression\n",
    "def lasso_regression(X_train, y_train, X_test):\n",
    "    model = Lasso(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# ElasticNet Regression\n",
    "def elastic_net_regression(X_train, y_train, X_test):\n",
    "    model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Decision Tree Regression\n",
    "def decision_tree_regression(X_train, y_train, X_test):\n",
    "    model = DecisionTreeRegressor(max_depth=None, min_samples_split=2)\n",
    "    # max_depth: Maximum depth of the tree. Controls the level of complexity.\n",
    "    # min_samples_split: Minimum number of samples required to split an internal node.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Random Forest Regression\n",
    "def random_forest_regression(X_train, y_train, X_test):\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_split=2)\n",
    "    # n_estimators: Number of trees in the forest.\n",
    "    # max_depth: Maximum depth of each tree in the forest.\n",
    "    # min_samples_split: Minimum number of samples required to split an internal node.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Gradient Boosting Regression\n",
    "def gradient_boosting_regression(X_train, y_train, X_test):\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    # n_estimators: Number of boosting stages (trees) to be built.\n",
    "    # learning_rate: Controls the contribution of each tree to the final prediction.\n",
    "    # max_depth: Maximum depth of each tree in the ensemble.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# XGBoost Regression\n",
    "def xgboost_regression(X_train, y_train, X_test):\n",
    "    model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    # n_estimators: Number of boosting stages (trees) to be built.\n",
    "    # learning_rate: Controls the contribution of each tree to the final prediction.\n",
    "    # max_depth: Maximum depth of each tree in the ensemble.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# LightGBM Regression\n",
    "def lightgbm_regression(X_train, y_train, X_test):\n",
    "    model = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    # n_estimators: Number of boosting stages (trees) to be built.\n",
    "    # learning_rate: Controls the contribution of each tree to the final prediction.\n",
    "    # max_depth: Maximum depth of each tree in the ensemble.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# CatBoost Regression\n",
    "def catboost_regression(X_train, y_train, X_test):\n",
    "    model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=3)\n",
    "    # iterations: Number of boosting stages (trees) to be built.\n",
    "    # learning_rate: Controls the contribution of each tree to the final prediction.\n",
    "    # depth: Maximum depth of each tree in the ensemble.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Support Vector Regression\n",
    "def svr_regression(X_train, y_train, X_test):\n",
    "    model = SVR(kernel='rbf', C=1.0)\n",
    "    # kernel: Specifies the kernel type used in the algorithm.\n",
    "    # C: Regularization parameter. Controls the trade-off between fitting to the data and allowing margin violations.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# K-Nearest Neighbors Regression\n",
    "def knn_regression(X_train, y_train, X_test):\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    # n_neighbors: Number of neighbors to use for prediction.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Neural Network Regression\n",
    "def neural_network_regression(X_train, y_train, X_test):\n",
    "    model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, alpha=0.0001)\n",
    "    # hidden_layer_sizes: Tuple representing the number of neurons in each hidden layer.\n",
    "    # max_iter: Maximum number of iterations to converge.\n",
    "    # alpha: L2 regularization term.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Bayesian Regression\n",
    "# BayesianRidge uses Bayesian principles to estimate the posterior distribution of model parameters and predictions.\n",
    "def bayesian_regression(X_train, y_train, X_test):\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Huber Regression\n",
    "# HuberRegressor is a robust regression technique that's less sensitive to outliers compared to ordinary least squares.\n",
    "def huber_regression(X_train, y_train, X_test):\n",
    "    model = HuberRegressor(epsilon=1.35)\n",
    "    # epsilon: Determines the threshold for outlier detection. Smaller values make the model more robust.\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Quantile Regression\n",
    "# QuantileRegressor focuses on modeling different quantiles of the target variable, useful for understanding the entire distribution.\n",
    "def quantile_regression(X_train, y_train, X_test):\n",
    "    model = QuantileRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Isotonic Regression\n",
    "# IsotonicRegression preserves the order of the data while modeling the relationship between features and target.\n",
    "# def isotonic_regression(X_train, y_train, X_test):\n",
    "#     model = IsotonicRegression(out_of_bounds='clip')\n",
    "#     # out_of_bounds: Determines how values outside the training domain are handled. 'clip' restricts predictions to the training range.\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return y_pred\n",
    "\n",
    "# Evaluate Models\n",
    "def evaluate_models(y_true, y_preds):\n",
    "    for model_name, y_pred in y_preds.items():\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        print(f\"{model_name} MSE: {mse:.4f}\")\n",
    "\n",
    "# Perform predictions and evaluate\n",
    "y_preds = {\n",
    "    'Linear Regression': linear_regression(X_train, y_train, X_test),\n",
    "    'Ridge Regression': ridge_regression(X_train, y_train, X_test),\n",
    "    'Lasso Regression': lasso_regression(X_train, y_train, X_test),\n",
    "    'ElasticNet Regression': elastic_net_regression(X_train, y_train, X_test),\n",
    "    'Decision Tree Regression': decision_tree_regression(X_train, y_train, X_test),\n",
    "    'Random Forest Regression': random_forest_regression(X_train, y_train, X_test),\n",
    "    'Gradient Boosting Regression': gradient_boosting_regression(X_train, y_train, X_test),\n",
    "    'XGBoost Regression': xgboost_regression(X_train, y_train, X_test),\n",
    "    'LightGBM Regression': lightgbm_regression(X_train, y_train, X_test),\n",
    "    'CatBoost Regression': catboost_regression(X_train, y_train, X_test),\n",
    "    'SVR Regression': svr_regression(X_train, y_train, X_test),\n",
    "    'KNN Regression': knn_regression(X_train, y_train, X_test),\n",
    "    'Neural Network Regression': neural_network_regression(X_train, y_train, X_test),\n",
    "    'Bayesian Regression': bayesian_regression(X_train, y_train, X_test),\n",
    "    'Huber Regression': huber_regression(X_train, y_train, X_test),\n",
    "    'Quantile Regression': quantile_regression(X_train, y_train, X_test),\n",
    "    #'Isotonic Regression': isotonic_regression(X_train, y_train, X_test)\n",
    "}\n",
    "\n",
    "# Evaluate and print results\n",
    "evaluate_models(y_test, y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
